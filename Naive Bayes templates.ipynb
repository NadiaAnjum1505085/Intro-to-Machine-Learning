{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category features\n",
    "continuous_features  = ['age', 'chol', 'oldpeak', 'thalch', 'trestbps']\n",
    "categorical_features = ['ca', 'cp', 'restecg', 'slope', 'thal', 'sex', 'fbs', 'exang']\n",
    "label = ['num']\n",
    "updated_categorical_features = []\n",
    "\n",
    "#read training data, normalize continuous features using (X - mean) / std\n",
    "def readdata():\n",
    "    df = pd.read_csv('heart_disease_uci.csv')\n",
    "    \n",
    "    # replacing nan value with std \n",
    "    for col in continuous_features:\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "        \n",
    "    # normalization of continuous features\n",
    "    for col in continuous_features:\n",
    "        df[col] = (df[col] - df[col].mean()) / df[col].std()\n",
    "        \n",
    "    # replacing nan value with mode \n",
    "    for col in categorical_features:\n",
    "        mode_val = df[col].mode()[0]\n",
    "        df.loc[df[col].isna(), col] = mode_val\n",
    "    df[categorical_features] = df[categorical_features].astype(\"category\")\n",
    "    \n",
    "    # replacing nan value with mode \n",
    "    for col in label:\n",
    "        mode_val = df[col].mode()[0]\n",
    "        df.loc[df[col].isna(), col] = mode_val\n",
    "        \n",
    "    # replacing 0 with negative, other values with positive\n",
    "    df['num'] = df['num'].apply(lambda x: \"positive\" if x > 0 else \"negative\")\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    df = pd.get_dummies(df, columns=categorical_features, prefix=categorical_features, drop_first=True)\n",
    "    for col in df.columns:\n",
    "        if col not in continuous_features and col != 'num' and col != 'id' and col != 'dataset':\n",
    "            updated_categorical_features.append(col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# separate data into training and testing sets\n",
    "def dataDivision():\n",
    "    new_df = readdata()\n",
    "    shuffled_df = new_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    label_col = 'num'\n",
    "    n = len(shuffled_df)\n",
    "\n",
    "    # separating data for test and training\n",
    "    train_length = int(0.8 * n)\n",
    "    train_df = shuffled_df.iloc[:train_length]\n",
    "    test_df = shuffled_df.iloc[train_length:]\n",
    "    train_df_mod = train_df.drop(columns=['id','dataset'])\n",
    "    test_df_mod = test_df.drop(columns=['id','dataset'])\n",
    "    \n",
    "    #print(\"Columns in DataFrame:\", train_df_mod.columns.tolist())\n",
    "    \n",
    "    return train_df_mod,test_df_mod\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB algorithm\n",
    "\n",
    "#calculate categorical feature probability\n",
    "def calcCatProb(train_df):\n",
    "    catagorical_prob = {'positive': {}, 'negative': {}}\n",
    "    classes = ['positive', 'negative']\n",
    "    \n",
    "    for cls in classes:\n",
    "        class_df = train_df[train_df['num'] == cls]\n",
    "        total_class = len(class_df)\n",
    "        for col in train_df.columns:\n",
    "            if col not in continuous_features and col != 'num':\n",
    "                probability = class_df[col].sum()/total_class\n",
    "                catagorical_prob[cls][col] = float(probability)\n",
    "    #print(catagorical_prob)\n",
    "    return catagorical_prob\n",
    "            \n",
    "\n",
    "#calculate continuous feature probability using Maximum Likelihood estimator for Gaussian Distribution\n",
    "def calcGaussProb(train_df):\n",
    "    gaussian_params = {'positive': {}, 'negative': {}}\n",
    "    classes = ['positive', 'negative']\n",
    "    \n",
    "\n",
    "    for cls in classes:\n",
    "        class_df = train_df[train_df['num'] == cls]\n",
    "        for col in train_df.columns:\n",
    "            if col in continuous_features:\n",
    "                mean_value = class_df[col].mean()\n",
    "                std_value = class_df[col].std()\n",
    "                gaussian_params[cls][col] = (mean_value,std_value)\n",
    "    #print(\"gaussian_params\",gaussian_params )\n",
    "    return gaussian_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes classifier\n",
    "def naiveBayes(train_df, test_df):\n",
    "    catProb = calcCatProb(train_df)\n",
    "    gaussProb = calcGaussProb(train_df)\n",
    "    classes = ['positive', 'negative']\n",
    "    priorProb = {'positive': sum(train_df['num'] == 'positive') / len(train_df),\n",
    "                 'negative': sum(train_df['num'] == 'negative') / len(train_df)\n",
    "                }\n",
    "\n",
    "\n",
    "    y_prediction = []\n",
    "    \n",
    "    for i in range((len(test_df))):\n",
    "        sample = test_df.iloc[i]\n",
    "        logProbs = {}\n",
    "        for cls in classes:\n",
    "            log_prob = math.log(priorProb[cls])\n",
    "            \n",
    "            for col in test_df.columns:\n",
    "                if col not in continuous_features and col != 'num':\n",
    "                    prob = catProb[cls].get(col, 1e-6)\n",
    "                    if sample[col] == 1:\n",
    "                        log_prob += math.log(prob)\n",
    "                    else:\n",
    "                        log_prob += math.log(1 - prob)\n",
    "            \n",
    "            for col in continuous_features:\n",
    "                mean, std = gaussProb[cls][col]\n",
    "                x = sample[col]\n",
    "                prob = (1 / (np.sqrt(2 * np.pi) * std)) * np.exp(-((x - mean) ** 2) / (2 * std ** 2))\n",
    "                log_prob += math.log(prob)\n",
    "\n",
    "            logProbs[cls] = log_prob\n",
    "            \n",
    "        y_prediction.append(max(logProbs, key=logProbs.get))\n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run naive bayes\n",
    "#naiveBayes(train, test)\n",
    "train_df,test_df = dataDivision()\n",
    "y_prediction = naiveBayes(train_df, test_df)\n",
    "y_true = test_df['num'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               Actual\n",
      "             1       0\n",
      "P       +--------+--------+\n",
      "r     1 | TP=85  | FP=26  |\n",
      "e       +--------+--------+\n",
      "d     0 | FN=13  | TN=60  |\n",
      ".       +--------+--------+\n",
      "\n",
      "Accuracy:   0.78804\n",
      "Precison:   0.76577\n",
      "Recall:     0.86735\n",
      "F-Measure:  0.81340\n"
     ]
    }
   ],
   "source": [
    "# Print results of Naive Bayes classifier\n",
    "\n",
    "def printConfusionMatrix(tp, fp, tn, fn):\n",
    "    print(\"\\n%15sActual\" % \"\")\n",
    "    print(\"%6s %7s %7s\" % (\"\", \"1\", \"0\"))\n",
    "    print(\"P%6s +--------+--------+\" % \"\")\n",
    "    print(\"r%6s | %-6s | %-6s |\" % (\"1\", 'TP='+str(tp), 'FP='+str(fp)))\n",
    "    print(\"e%6s +--------+--------+\" % \"\")\n",
    "    print(\"d%6s | %-6s | %-6s |\" % (\"0\", 'FN='+str(fn), 'TN='+str(tn)))\n",
    "    print(\".%6s +--------+--------+\\n\" % \"\")\n",
    "\n",
    "def getConfusionMatrix(y_true, y_pred):\n",
    "    tp = fp = tn = fn = 0\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        if t == 'positive' and p == 'positive': tp += 1\n",
    "        if t == 'negative' and p == 'positive': fp += 1\n",
    "        if t == 'negative' and p == 'negative': tn += 1\n",
    "        if t == 'positive' and p == 'negative': fn += 1\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "def getAccuracy(tp, fp, tn, fn):\n",
    "    return (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "def getPrecision(tp, fp, tn, fn):\n",
    "    return tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "\n",
    "def getRecall(tp, fp, tn, fn):\n",
    "    return tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "def getFScore(tp, fp, tn, fn):\n",
    "    precision = getPrecision(tp, fp, tn, fn)\n",
    "    recall = getRecall(tp, fp, tn, fn)\n",
    "    return 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "tp, fp, tn, fn = getConfusionMatrix(y_true, y_prediction)\n",
    "printConfusionMatrix(tp, fp, tn, fn)\n",
    "\n",
    "\n",
    "print('Accuracy:  %8.5f' % getAccuracy(tp, fp, tn, fn))\n",
    "print('Precison:  %8.5f' % getPrecision(tp, fp, tn, fn))\n",
    "print('Recall:    %8.5f' % getRecall(tp, fp, tn, fn))\n",
    "print('F-Measure: %8.5f' % getFScore(tp, fp, tn, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
